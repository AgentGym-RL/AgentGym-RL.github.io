<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8" />
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="AgentGym" />

  <!-- Keywords for your paper to be indexed by-->
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>
    AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making through Multi-Turn Reinforcement Learning
  </title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico" />
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet" />

  <link rel="stylesheet" href="static/css/bulma.min.css" />
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css" />
  <link rel="stylesheet" href="static/css/bulma-slider.min.css" />
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" />
  <link rel="stylesheet" href="static/css/index.css" />

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">
              AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making through Multi-Turn Reinforcement Learning
            </h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://woooodyy.github.io/" target="_blank" rel="noopener noreferrer">Zhiheng Xi</a>,
              </span>
              <span class="author-block">
                Jixuan Huang,
              </span>
              <span class="author-block">
                Chenyang Liao,
              </span>
              <span class="author-block">
                Baodai Huang,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=HrYYk4YAAAAJ" target="_blank"
                  rel="noopener noreferrer">Honglin Guo</a>,
              </span>
              <span class="author-block">
                Jiaqi Liu,
              </span>
              <span class="author-block">
                Rui Zheng,
              </span>
              <span class="author-block">
                Junjie Ye,
              </span>
              <span class="author-block">
                Jiazheng Zhang,
              </span>
              <span class="author-block">
                Wenxiang Chen,
              </span>
              <span class="author-block">
                Wei He,
              </span>
              <span class="author-block">
                Yiwen Ding,
              </span>
              <span class="author-block">
                Guanyu Li,
              </span>
              <span class="author-block">
                Zehui Chen,
              </span>
              <span class="author-block">
                Zhengyin Du,
              </span>
              <span class="author-block">
                Xuesong Yao,
              </span>
              <span class="author-block">
                Yufei Xu,
              </span>
              <span class="author-block">
                Jiecao Chen,
              </span>
              <span class="author-block">
                <a href="https://guitaowufeng.github.io/" target="_blank" rel="noopener noreferrer">Tao Gui</a>,
              </span>
              <span class="author-block">
                <a href="https://zxwu.azurewebsites.net/" target="_blank" rel="noopener noreferrer">Zuxuan Wu</a>,
              </span>
              <span class="author-block">
                <a href="http://qizhang.info/" target="_blank" rel="noopener noreferrer">Qi Zhang</a>,
              </span>
              <span class="author-block">
                <a href="https://xuanjing-huang.github.io/" target="_blank" rel="noopener noreferrer">Xuanjing
                  Huang</a>
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=f3_FP8AAAAAJ&hl=en" target="_blank"
                  rel="noopener noreferrer">Yu-Gang Jiang</a>,
              </span>
              <!-- <span class="author-block">
                Yiwen Ding,
              </span>
              <span class="author-block">
                Wenxiang Chen,
              </span>
              <span class="author-block">
                Boyang Hong,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=HrYYk4YAAAAJ" target="_blank"
                  rel="noopener noreferrer">Honglin Guo</a>,
              </span>
              <span class="author-block">
                Junzhe Wang,
              </span>
              <span class="author-block">
                Dingwen Yang,
              </span>
              <span class="author-block">
                Chenyang Liao,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=TwzyqksAAAAJ" target="_blank"
                  rel="noopener noreferrer">Xin Guo</a>,
              </span>
              <span class="author-block">
                Wei He,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=O42mLrsAAAAJ" target="_blank"
                  rel="noopener noreferrer">Songyang Gao</a>,
              </span>
              <span class="author-block">
                Lu Chen,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=7Z0V_SoAAAAJ&hl=en" target="_blank"
                  rel="noopener noreferrer">Rui Zheng</a>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=X_nKjOYAAAAJ&hl=en" target="_blank"
                  rel="noopener noreferrer">Yicheng Zou</a>,
              </span>
              </br>
              <span class="author-block">
                <a href="https://guitaowufeng.github.io/" target="_blank" rel="noopener noreferrer">Tao Gui</a>,
              </span>
              <span class="author-block">
                <a href="http://qizhang.info/" target="_blank" rel="noopener noreferrer">Qi Zhang</a>,
              </span>
              <span class="author-block">
                <a href="https://xpqiu.github.io/" target="_blank" rel="noopener noreferrer">Xipeng Qiu</a>,
              </span>
              <span class="author-block">
                <a href="https://xuanjing-huang.github.io/" target="_blank" rel="noopener noreferrer">Xuanjing
                  Huang</a>,
              </span>
              <span class="author-block">
                <a href="https://zxwu.azurewebsites.net/" target="_blank" rel="noopener noreferrer">Zuxuan Wu</a>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=f3_FP8AAAAAJ&hl=en" target="_blank"
                  rel="noopener noreferrer">Yu-Gang Jiang</a>,
              </span> -->
            </div>


            <div class="is-size-5 publication-authors">
              <span class="author-block" style="font-size: 24px;">Fudan University & ByteDance Seed & Shanghai Innovation Institute</span>
              <!-- <span class="eql-cntrb"><small><br /><sup>*</sup>Indicates Equal Contribution</small></span> -->
              </br>
              <span class="author-block">Correspondence to: zhxi22@m.fudan.edu.cn , {tgui,qz}@fudan.edu.cn</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2406.04151" target="_blank"
                    class="external-link button is-normal is-rounded is-dark" rel="noopener noreferrer">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/WooooDyy/AgentGym" target="_blank"
                    class="external-link button is-normal is-rounded is-dark" rel="noopener noreferrer">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- Model -->
                <!-- <span class="link-block">
                  <a href="https://huggingface.co/AgentGym/AgentEvol-7B" target="_blank"
                    class="external-link button is-normal is-rounded is-dark" rel="noopener noreferrer">
                    <span class="icon">
                      ðŸ¤—
                    </span>
                    <span>Model: AgentRL</span>
                  </a>
                </span> -->

                <!-- Dataset -->
                <span class="link-block">
                  <a href=" https://huggingface.co/datasets/AgentGym/AgentGym-RL-Data-ID" target="_blank"
                    class="external-link button is-normal is-rounded is-dark" rel="noopener noreferrer">
                    <span class="icon">
                      ðŸ¤—
                    </span>
                    <span>AgentGym-RL-Data-ID</span>
                  </a>
                </span>

                <!-- Dataset -->
                <!-- <span class="link-block">
                  <a href="https://huggingface.co/datasets/AgentGym/AgentEval" target="_blank"
                    class="external-link button is-normal is-rounded is-dark" rel="noopener noreferrer">
                    <span class="icon">
                      ðŸ¤—
                    </span>
                    <span>AgentEval</span>
                  </a>
                </span> -->
                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2406.04151" target="_blank"
                    class="external-link button is-normal is-rounded is-dark" rel="noopener noreferrer">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>



  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Developing autonomous LLM agents capable of making a series of intelligent decisions to solve complex, real-world tasks is a fast-evolving frontier. Like human cognitive development, agents are expected to acquire knowledge and skills through exploration and interaction with the environment.
              Despite advances, the community still lacks a unified, interactive reinforcement learning (RL) framework that can effectively train such agents from scratchâ€”without relying on supervised fine-tuning (SFT)â€”across diverse and realistic environments.
              To bridge this gap, we introduce AgentGym-RL, a new framework to train LLM agents for multi-turn interactive decision-making through RL. 
              The framework features a modular and decoupled architecture, ensuring high flexibility and extensibility. It encompasses a wide variety of real-world scenarios, and supports mainstream RL algorithms. Furthermore, we propose ScalingInter-RL, a training approach designed for exploration-exploitation balance and stable RL optimization. In early stages, it emphasizes exploitation by restricting the number of interaction, and gradually shifts towards exploration with larger horizons to encourage diverse problem-solving strategies. In this way, the agent develops more diverse behaviors and is less prone to collapse under long horizons.
              We perform extensive experiments to validate the stability and effectiveness of both the AgentGym-RL framework and ScalingInter-RL approach. Our agents match or surpass commercial models on 27 tasks across diverse environments. We offer key insights and will open-source complete AgentGym-RL frameworkâ€”including code and datasetsâ€”to empower the research community in developing the next generation of intelligent agents.
            </p>
          </div>
        </div>
      </div>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="content has-text-justified">
            <img src="static\images\AgentGym-RL-main.png" alt="MY ALT TEXT" />
          </div>
          <p>
            <b>Figure 1:</b> Overview of the AgentGym-RL framework. It features a decoupled, flexible, and extensible architecture, comprising three primary modulesâ€”the environment, the agent, and the training module. It supports diverse scenarios, environments, and algorithms.
          </p>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->

  <section class="section hero is-light">
    <div class="container is-max-desktop">

    </div>
  </section>


  <section class="section hero">
    <div class="container is-max-desktop content">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Diverse Scenarios and Environments</h2>
          
          <!-- <div class="columns is-centered">
              <div class="column is-four-fifths"> -->
                  <div class="content has-text-justified">
                      <p>
                        To build LLM agents capable of multi-turn sequential decision-making for complex tasks in real-world environments, AgentGym-RL covers a broad spectrum of scenarios to comprehensively evaluate and foster the agent's ability to perceive its environment, long-term planning towards a goal, in-depth reasoning for making intelligent decisions, aptitude for reflection and correction when facing setbacks or making mistakes.
                    </p>
                    <ul>
                        <li><b>Web Navigation</b>: Interacting with dynamic websites for tasks such as booking flights or extracting structured information, which requires agents to follow instructions, interpret textual and visual content, manipulate dynamic interfaces, and plan multi-step actions.</li>
                        <li><b>Deep Search</b>: Performing multi-step, goal-directed queries with tools like browsers or Python interpreters, demanding strong information-seeking, multi-hop reasoning, long-term memory, and knowledge synthesis across sources.</li>
                        <li><b>Digital Games</b>: Exploring and solving problems in interactive game-like environments, emphasizing real-time decision-making, strategy development, and adaptability to complex, dynamic settings.</li>
                        <li><b>Embodied Tasks</b>: Controlling virtual or physical bodies for navigation, manipulation, and task execution, which calls for goal-directed planning, spatial reasoning, and robust perceptionâ€“action grounding.</li>
                        <li><b>Scientific Tasks</b>: Conducting experiments and solving problems in physically grounded, knowledge-intensive settings, requiring precise execution, dynamic interpretation of feedback, evidence-based reasoning, and iterative hypothesis refinement.</li>
                    </ul>
                    <img src="static/images/env.png" alt="An overview of the visualized user interface of our framework" />
                     <p class="has-text-centered">
                        <b>Figure 2:</b> An overview of the visualized user interface of our framework, facilitating observability and analysis.
                    </p>
                    <!-- Video carousel -->
                    <section class="hero is-small">
                      <div class="hero-body">
                        <div class="container">
                          <h2 class="title is-4 has-text-centered">Visualization Demo Videos</h2>
                          <div id="results-carousel" class="carousel results-carousel">
                            <div class="item item-video1">
                              <video poster="" id="video1" autoplay controls muted loop height="100%">
                                <!-- Your video file here -->
                                <source src="static/videos/webarena.mp4"
                                type="video/mp4">
                              </video>
                              <h2 class="subtitle has-text-centered">
                                WebArena: Multi-step web navigation.
                              </h2>
                            </div>
                            <div class="item item-video2">
                              <video poster="" id="video2" autoplay controls muted loop height="100%">
                                <!-- Your video file here -->
                                <source src="static/videos/searchqa.mp4"
                                type="video/mp4">
                              </video>
                              <h2 class="subtitle has-text-centered">
                                SearchQA: Deep multi-hop information seeking.
                              </h2>
                            </div>
                            <div class="item item-video3">
                              <video poster="" id="video3" autoplay controls muted loop height="100%">\
                                <!-- Your video file here -->
                                <source src="static/videos/textcraft.mp4"
                                type="video/mp4">
                              </video>
                              <h2 class="subtitle has-text-centered">
                                TextCraft: Goal-driven reasoning and planning in a text-based minecraft world.
                              </h2>
                            </div>
                            <div class="item item-video4">
                              <video poster="" id="video4" autoplay controls muted loop height="100%">\
                                <!-- Your video file here -->
                                <source src="static/videos/babyai.mp4"
                                type="video/mp4">
                              </video>
                              <h2 class="subtitle has-text-centered">
                                BabyAI: Navigation and sequential action execution in a gridworld environment.
                              </h2>
                            </div>
                            <div class="item item-video5">
                              <video poster="" id="video5" autoplay controls muted loop height="100%">\
                                <!-- Your video file here -->
                                <source src="static/videos/sciworld.mp4"
                                type="video/mp4">
                              </video>
                              <h2 class="subtitle has-text-centered">
                                SciWorld: Scientific task solving.
                              </h2>
                            </div>
                          </div>
                        </div>
                      </div>
                    </section>
                    <!-- End video carousel -->
                </div>
            </div>
        </div>
    </div>
  </section>

    <!-- Method -->
  <section class="section">
    <div class="container is-max-desktop content">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
        <h2 class="title is-3">ScalingInter-RL Method</h2>
        <!-- <div class="columns is-centered">
            <div class="column is-four-fifths"> -->
                <div class="content has-text-justified">
                    <p>
                        Beyond relying on internal reasoning to select the next action, agents should also expand their external interactions with the environment to ensure sufficient exploration and accumulate richer context toward the final goalâ€”capturing a form of practice-driven insight. Yet, our preliminary experiments indicate that beginning with a large number of interaction turns often leads the model into redundant reasoning and unproductive actions, ultimately causing training collapse and degraded performance. Conversely, constraining the number of interactions to remain consistently small tends to narrow exploration and limits the agentâ€™s ability to master diverse patterns. This motivates us to propose our ScalingInter-RL method.
                    </p>
                    <img src="static\images\ScalingInter-RL-Method.png" alt="Illustration of the ScalingInter-RL approach" />
                    <p class="has-text-centered">
                        <b>Figure 3:</b> Illustration of the ScalingInter-RL approach. It allows the agent to adapt in stages: initially, by limiting interaction turns to prioritize exploitation, master basic skills, and solve easy tasks; later, by gradually increasing interactions to explore, avoid shortcuts, refine behavior, and tackle harder problems. Ultimately, this process trains a stronger agent.
                    </p>
                </div>
            </div>
        </div>
    </div>
  </section>
  <!-- End Method -->

  <!---->

  <section class="section hero">
    <div class="container is-max-desktop content">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Performance</h2>
          <div class="content has-text-justified">
          <p>
          We leverage Qwen2.5-3B and Qwen2.5-7B as our primary backbone models. We evaludate AgentGym-RL and ScalingInter-RL across 5 scenarios and include multiple closed-source models and open-source models for comparison. The results are as follows.
          </p>
          <img src="static\images\performance.png" alt="Performance Results" >
          <p class="has-text-centered">
            <b>Figure 4:</b> Performance comparison of our AgentGym-RL trained agents against various baselines across five scenarios. AgentGym-RL-7B outperforms other open-source models by a large margin.
          </p>
          <p class="content has-text-justified">
            The AgentGym-RL framework and method substantially enhances the open-sourced 7B-scale models' capabilities to a level that rivals or even surpasses top-tier proprietary large models.
            Moreover, ScalingInter-RL demonstrates more stable and efficient training dynamics during RL optimization as shown in the figure below.
          </p>
          <img src="static\images\ScalingInter-RL-performance.png" alt="ScalingInter-RL Dynamics" />
        </div>
      </div>
    </div>
  </section>

  <!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @misc{xi2025agentgymrl,
          title={AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making through Multi-Turn Reinforcement Learning}, 
          author={Zhiheng Xi, Jixuan Huang, Chenyang Liao, Baodai Huang, Honglin Guo, Jiaqi Liu, Rui Zheng, Junjie Ye, Jiazheng Zhang, Wenxiang Chen, Wei He, Yiwen Ding, Guanyu Li, Zehui Chen, Zhengyin Du, Xuesong Yao, Yufei Xu, Jiecao Chen, Tao Gui, Zuxuan Wu, Qi Zhang, Xuanjing Huang, Yu-Gang Jiang},
          year={2025},
          eprint={TODO},
          archivePrefix={arXiv},
          primaryClass={cs.AI}
          }
      </code></pre>
    </div>
  </section>
  <!--End BibTex citation -->

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This page was built using the
              <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank"
                rel="noopener noreferrer">Academic Project Page Template</a>
              which was adopted from the <a href="https://nerfies.github.io" target="_blank"
                rel="noopener noreferrer">Nerfies</a> project page. You are free to borrow the of this
              website, we
              just ask that you link back to this page in the footer. <br />
              This website is licensed under a
              <a rel="license  noopener noreferrer" href="http://creativecommons.org/licenses/by-sa/4.0/"
                target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International
                License</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->
</body>

</html>